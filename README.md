# ictir23-cost-sensitive

# New Insights into Metric Optimization for Ranking-based Recommendation

This is our implementation and experimental data for the paper:

Roger Zhe Li, Juli√°n Urbano, Alan Hanjalic (2023). Mitigating Mainstream Bias in Recommendation via Cost-sensitive Learning. In Proceedings of ICITR'23, Taipei, Taiwan, July 23, 2023.

**Please cite our SIGIR'21 paper if you use our code and data. Thanks!** 

Author: Roger Zhe Li (https://www.zhe-li.me)

## Environment Settings
We use PyTorch 1.6.0 as the main deep learning framework for implementation. <br/>
The debugging stage relies much on the Torchsnooper package. <br/>
The dataset processing and splitting stage is conducted with the dependency of [PyLensKit](https://lenskit.org/). 

Figures and related analysis in the paper are mainly implemented in plotnine, a Python package achieving the similar effects as ggplot2 in R.


## File and Folder Structure

[./cost_sensitive_func.ipynb](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/cost_sensitive_func.ipynb): An indicator of the truncated normal distribution-based cost-sensitive weights, and the method in controlling the contrast of the importance of most and least non-mainstream users. Numbers calculated are further used in the core runs. <br/>

[./analysis/movielens.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/analysis/movielens.py): processing movielens dataset; <br/>
[./analysis/Amazon.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/analysis/Amazon.py): processing Amazon dataset; <br/>
[./analysis/beer.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/analysis/beer.py): processing Amazon datasets. <br/>
[./dataset_prep.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/analysis/dataset_prep.py): script for train-val-test data splits on different conditions. <br/>

[./dataset](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/dataset): Store the processed raw files, preprocessed tables, and the train-val-test split subsets of datasets; <br/>
[./results](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/results_overall): Store the aggregated results of all experiments including the effectiveness of cost-sensitive learning and explorations on the data split needed for reliable results. Aggregated from files in ./results_random which could be generated by running the source code. Each file is pointed to one dataset. <br/>

All other .py files are related to the experiments and result analysis. See below for a brief introduction.

[./vanilla.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/vanilla.py): source code for running Factorization Machines to get the prerequisites for caculating cost-sensitive weights; <br/>
[./data_loader_random.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/data_loader_random.py): data loader module for training/validation/testing; <br/>
[./csl_random.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/csl_random.py): Souce code for cost-sensitive training and validation;  <br/>
[./eval_random.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/eval_random.py): model evaluation;  <br/>
[./corr_valid_test.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/core_valid_test.py): correlation analysis for different numbers of items used for validation and testing;  <br/>
[./stat_line.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/stat_line.py): visualization of correlation analysis mentioned above; <br/>
[./data_aggregation.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/data_aggregation.py) put all experimental results together, one for each dataset; <br/>
[./plot_csl.py](https://github.com/roger-zhe-li/ictir23-cost-sensitive/tree/main/plot_csl.py): visualization of the artifacts of cost-sensitive strategies. <br/>


## Example to run the code
The instruction of commands has been clearly stated in the code (see the parse_args function under ./util/parser.py). All random seeds are by defualt 8964. 

Run example (after all data processing is done):

Step 1: get the vanilla results from FM
```
python3 vanilla.py --dataset movielens --train_low 5 --valid_low 5 --seed 8964
```

Steps 2: cost-sensitive learning:
```
python3 csl_random.py --dataset movielens --train_low 5 --valid_low 5 --seed 8964 --norm quantile --sigma_type 0
```


### Dataset
We provide four processed datasets with three different train-test splits under the ./dataset folder. The processing methods are stated in the paper. 

The original [movielens](https://github.com/roger-zhe-li/ictir23-cost-sensitive/blob/main/data/citeulike/users.dat) dataset is available in this repo. For two Amazon datasets, the original version is too large to be available here. You can find them [here](https://nijianmo.github.io/amazon/index.html). As per the request from the data providers, BeerAdvocate dataset is no longer public.


### Cite

Please cite our ICTIR'21 paper if you use the code.

```
```


## License
* The paper is licensed under a [Creative Commons Attribution International 4.0 License](https://creativecommons.org/licenses/by/4.0/).
* Databases and their contents are distributed under the terms of the [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) unless there is an explicit revert from the data provider..
* Software is distributed under the terms of the [MIT License](https://opensource.org/licenses/MIT).



Last Update Date: June 27, 2023
